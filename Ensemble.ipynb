{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ENSEMBLE LEARNING"
      ],
      "metadata": {
        "id": "Wo8EHR8Jz_0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Can we use Bagging for regression problems?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Yes.\n",
        "    Bagging can be applied to regression and classification.\n",
        "    In regression, common base estimators:\n",
        "\n",
        "    DecisionTreeRegressor\n",
        "\n",
        "    KNeighborsRegressor\n",
        "\n",
        "    Bagging reduces variance and improves stability.\n",
        "\n",
        "2.What is the difference between multiple model training and single model training?  \n",
        "\n",
        "ans-\n",
        "\n",
        "  The difference between multiple model training and single model training as follows\n",
        "\n",
        "    Single Model\n",
        "    Uses one algorithm\n",
        "    High risks of overfitting\n",
        "    Low computational cost\n",
        "    Limited accuracy\n",
        "\n",
        "    Multiple Models (Ensemble)\n",
        "\t  Trains many models\n",
        "\t  Reduces overfitting (Bagging)\n",
        "\t  Higher computational cost\n",
        "\t  Higher accuracy & robustness\n",
        "\n",
        "3.Explain the concept of feature randomness in Random Forest?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Random Forest introduces feature randomness by selecting a random subset of features at each split.\n",
        "    This reduces correlation among trees and improves generalization.    \n",
        "\n",
        "4.What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    In bagging, each tree is trained on a bootstrap sample (about 63% of data).\n",
        "    The remaining 37% is Out-of-Bag data.\n",
        "    OOB score = accuracy evaluated on this leftover data.\n",
        "    It acts like built-in cross-validation.    \n",
        "\n",
        "\n",
        "5.How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    Two main methods:\n",
        "\n",
        "    Gini Importance / Mean Decrease in Impurity\n",
        "\n",
        "    Permutation Importance (more accurate)\n",
        "\n",
        "6.Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "ans-\n",
        "\n",
        "    Steps:\n",
        "\n",
        "    Create many bootstrap samples.\n",
        "\n",
        "    Train a base model on each sample.\n",
        "\n",
        "    Combine predictions using majority vote (classification).  \n",
        "\n",
        "7.How to evaluate Bagging Classifier performance?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Accuracy\n",
        "\n",
        "    Confusion Matrix\n",
        "\n",
        "    Precision, Recall, F1\n",
        "\n",
        "    ROC-AUC\n",
        "\n",
        "    Cross-validation\n",
        "\n",
        "    OOB Score\n",
        "\n",
        "8.How Bagging Regressor works?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Same as classifier but uses:\n",
        "\n",
        "    Averaging of predictions instead of voting.\n",
        "\n",
        "9.What is the main advantage of ensemble techniques?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    Higher accuracy\n",
        "\n",
        "    Reduced variance\n",
        "\n",
        "    More robust than single model\n",
        "\n",
        "10.What is the main challenge of ensemble methods?\n",
        "\n",
        "ans-\n",
        "\n",
        "    High computational cost\n",
        "\n",
        "    Harder to interpret\n",
        "\n",
        "    Slower training & inference\n",
        "\n",
        "11.Explain the key idea behind ensemble techniques\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    \"Combine multiple weak models to form a strong, more accurate model.\"\n",
        "\n",
        "12.What is Random Forest Classifier?\n",
        "\n",
        "ans-\n",
        "\n",
        "    An ensemble of many decision trees, each trained on:\n",
        "\n",
        "    Random data (bootstrap)\n",
        "\n",
        "    Random features\n",
        "\n",
        "    Final prediction: majority vote.\n",
        "\n",
        "13.What are the main types of ensemble techniques?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    Bagging (Bootstrap Aggregation)\n",
        "\n",
        "    Boosting (AdaBoost, XGBoost, GradientBoosting)\n",
        "\n",
        "    Stacking\n",
        "\n",
        "    Voting\n",
        "\n",
        "14.What is ensemble learning?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    Using multiple models together to improve:\n",
        "\n",
        "    Accuracy\n",
        "\n",
        "    Robustness\n",
        "\n",
        "    Stability\n",
        "\n",
        "15.When should we avoid using ensemble methods?\n",
        "\n",
        "ans-\n",
        "\n",
        "\n",
        "    When model interpretability is required\n",
        "\n",
        "    Low compute environment\n",
        "\n",
        "    Small dataset\n",
        "\n",
        "    When a simple model performs equally well\n",
        "\n",
        "16.How does Bagging reduce overfitting?\n",
        "\n",
        "ans-\n",
        "\n",
        "    By training on different bootstrap samples, each model learns different patterns → reduces variance.\n",
        "\n",
        "17.Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Less overfitting\n",
        "\n",
        "    More stable\n",
        "\n",
        "    Better generalization\n",
        "\n",
        "    Uses random feature selection\n",
        "\n",
        "18.What is role of bootstrap sampling in Bagging?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Bootstrap sampling ensures:\n",
        "\n",
        "    Diversity among models\n",
        "\n",
        "    Variance reduction\n",
        "\n",
        "    OOB score calculation\n",
        "\n",
        "19.What are some real-world applications of ensemble techniques?\n",
        "\n",
        "ans-\n",
        "\n",
        "    Credit scoring\n",
        "\n",
        "    Fraud detection\n",
        "\n",
        "    Medical diagnosis\n",
        "\n",
        "    Stock market prediction\n",
        "\n",
        "    Recommendation systems\n",
        "\n",
        "    Image classification      \n",
        "\n",
        "20.What is the difference between Bagging and Boosting?\n",
        "\n",
        "ans-\n",
        "\n",
        "  Difference between Bagging and Boosting\n",
        "\n",
        "    Bagging\t               Boosting\n",
        "    Parallel training\t    Sequential training\n",
        "    Reduces variance\t    Reduces bias\n",
        "    Trees independent\t    Trees depend on previous\n",
        "    Equal weights\t        Higher  weights to misclassified samples\n"
      ],
      "metadata": {
        "id": "ikxb9_7d0N2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PRATICAL"
      ],
      "metadata": {
        "id": "UB5Z1xay9oyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "r-P6bODN9tWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO32uLUn95Rv",
        "outputId": "a92ca124-4ced-4959-9c2e-5c4e4c23f8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "29cSqkxC-ECy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y =fetch_california_housing (return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4X25yLy-MLL",
        "outputId": "1a993b4d-2a6d-41c7-b3ee-566142cf60b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.25208410925367186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "NRRepfBC-n4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X, y)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, score in zip(data.feature_names, rf.feature_importances_):\n",
        "    print(name, score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1_MHi-R-uTK",
        "outputId": "19fc0f2d-5c04-4da0-9c35-32ba1f1bd100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "mean radius 0.03899588881836073\n",
            "mean texture 0.012422720355285034\n",
            "mean perimeter 0.06726533904253855\n",
            "mean area 0.0385427259206664\n",
            "mean smoothness 0.007903692374138\n",
            "mean compactness 0.023505437738739844\n",
            "mean concavity 0.062262901187864726\n",
            "mean concave points 0.07607472376397445\n",
            "mean symmetry 0.005025884141590706\n",
            "mean fractal dimension 0.004611796208420696\n",
            "radius error 0.02154943712114706\n",
            "texture error 0.003948648261601671\n",
            "perimeter error 0.013732179861316278\n",
            "area error 0.033987841768938674\n",
            "smoothness error 0.004328236022753781\n",
            "compactness error 0.0042975725625022014\n",
            "concavity error 0.004162108280291648\n",
            "concave points error 0.004399425824947104\n",
            "symmetry error 0.003053966370179002\n",
            "fractal dimension error 0.004684142004418814\n",
            "worst radius 0.09986654496956344\n",
            "worst texture 0.019655743794358083\n",
            "worst perimeter 0.1644433758794576\n",
            "worst area 0.08273657171779758\n",
            "worst smoothness 0.016064207477033868\n",
            "worst compactness 0.019041934555162284\n",
            "worst concavity 0.024490041638707123\n",
            "worst concave points 0.11848189426000397\n",
            "worst symmetry 0.01154214628247854\n",
            "worst fractal dimension 0.008922871795762273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "BKI8b9k4-25i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
        "rf = RandomForestRegressor().fit(X_train, y_train)\n",
        "\n",
        "print(\"Tree Score:\", r2_score(y_test, tree.predict(X_test)))\n",
        "print(\"RF Score:\", r2_score(y_test, rf.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEGx3dE--9G9",
        "outputId": "45f00b3c-3321-4d8c-f268-469b8a1fcb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree Score: 0.6100765989965337\n",
            "RF Score: 0.8114639181353923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "0qT6rQ2y_NoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(oob_score=True)\n",
        "rf.fit(X, y)\n",
        "print(\"OOB Score:\", rf.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj8nePfL_SKV",
        "outputId": "b201db74-1528-4678-a863-03d2d2163656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOB Score: 0.961335676625659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Train a Bagging Classifier using SVM as a base estimator and print accuracy2"
      ],
      "metadata": {
        "id": "kyHICzQa_XGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a classification dataset\n",
        "X_clf, y_clf = load_iris(return_X_y=True)\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42)\n",
        "\n",
        "model = BaggingClassifier(estimator=SVC(), n_estimators=20, random_state=42)\n",
        "model.fit(X_train_clf, y_train_clf)\n",
        "\n",
        "pred_clf = model.predict(X_test_clf)\n",
        "print(\"Accuracy:\", accuracy_score(y_test_clf, pred_clf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvMWfwVT_kO5",
        "outputId": "098670c3-1c33-431b-b5aa-12d86a4514ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Train a Random Forest Classifier with different numbers of trees and compare accuracy2"
      ],
      "metadata": {
        "id": "_GXCh5aZ_58r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset for classification\n",
        "data_bc = load_breast_cancer()\n",
        "X_bc, y_bc = data_bc.data, data_bc.target\n",
        "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_bc, y_bc, test_size=0.3, random_state=42)\n",
        "\n",
        "for n in [10, 50, 100, 200]:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train_bc, y_train_bc)\n",
        "    print(f\"Number of trees: {n}, Accuracy: {rf.score(X_test_bc, y_test_bc)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwhogm31__lo",
        "outputId": "07ed034a-2a70-4a4b-9760-aaafc9c8771a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trees: 10, Accuracy: 0.9649122807017544\n",
            "Number of trees: 50, Accuracy: 0.9707602339181286\n",
            "Number of trees: 100, Accuracy: 0.9707602339181286\n",
            "Number of trees: 200, Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "kusA0XubA8ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Breast Cancer dataset for classification\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = BaggingClassifier(\n",
        "    estimator=LogisticRegression(max_iter=1000),\n",
        "    n_estimators=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"AUC:\", roc_auc_score(y_test, proba))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZmbAuoUEkYF",
        "outputId": "c90f52a3-b407-4d74-e3ea-3a468476daae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9979423868312757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.Train a Random Forest Regressor and analyze feature importance scores?"
      ],
      "metadata": {
        "id": "Nq4WgeFiEYWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data_housing = fetch_california_housing()\n",
        "X_housing, y_housing = data_housing.data, data_housing.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(X_housing, y_housing, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "rf_regressor.fit(X_train_housing, y_train_housing)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"Feature Importances for Random Forest Regressor:\")\n",
        "for name, score in zip(data_housing.feature_names, rf_regressor.feature_importances_):\n",
        "    print(f\"{name}: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVLD52jdF6Xp",
        "outputId": "8fe6d99b-db52-4200-f9d4-bc896d73be2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances for Random Forest Regressor:\n",
            "MedInc: 0.5260109842753186\n",
            "HouseAge: 0.05465403794347876\n",
            "AveRooms: 0.04718824566150784\n",
            "AveBedrms: 0.02999453307261222\n",
            "Population: 0.031721806473035755\n",
            "AveOccup: 0.13821986994513355\n",
            "Latitude: 0.08608613054646935\n",
            "Longitude: 0.08612439208244402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "156zBgYpGTIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50)\n",
        "rf = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test, y_test))\n",
        "print(\"Random Forest Accuracy:\", rf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NZ4LFrBGdFp",
        "outputId": "d0ad70a0-3cef-4912-d7f0-78a31bde381b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Accuracy: 0.9590643274853801\n",
            "Random Forest Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n"
      ],
      "metadata": {
        "id": "OBO2OgpWGi7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKeMNCDzGwHb",
        "outputId": "54f60c10-8041-452c-a4ab-d222d47b0803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'max_depth': 5, 'n_estimators': 50}\n",
            "Best Accuracy: 0.9572795625427205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32.Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "QmbIahxPG8Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset for regression\n",
        "X_reg, y_reg = fetch_california_housing(return_X_y=True)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "for n in [10, 50, 100, 200]:\n",
        "    model = BaggingRegressor(n_estimators=n, random_state=42)\n",
        "    model.fit(X_train_reg, y_train_reg)\n",
        "    print(f\"{n} Estimators → MSE: {mean_squared_error(y_test_reg, model.predict(X_test_reg))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf-NApWBHYFz",
        "outputId": "4e223897-a863-4542-e220-befcfa555495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Estimators → MSE: 0.28623579601385674\n",
            "50 Estimators → MSE: 0.25787382250585034\n",
            "100 Estimators → MSE: 0.2568358813508342\n",
            "200 Estimators → MSE: 0.2541650541215747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "IIKQ_XeqHy9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]\n",
        "\n",
        "print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
        "print(\"Misclassified sample details (Index, Actual, Predicted):\")\n",
        "for idx in misclassified_indices:\n",
        "    print(f\"  Sample Index: {idx}, Actual: {data.target_names[y_test[idx]]}, Predicted: {data.target_names[y_pred[idx]]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ynXv73IO8F",
        "outputId": "f943725a-1604-44e4-8e43-9e6175ece914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of misclassified samples: 5\n",
            "Misclassified sample details (Index, Actual, Predicted):\n",
            "  Sample Index: 8, Actual: benign, Predicted: malignant\n",
            "  Sample Index: 20, Actual: malignant, Predicted: benign\n",
            "  Sample Index: 77, Actual: malignant, Predicted: benign\n",
            "  Sample Index: 82, Actual: malignant, Predicted: benign\n",
            "  Sample Index: 164, Actual: malignant, Predicted: benign\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier="
      ],
      "metadata": {
        "id": "QLxMKxQ5IdIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "bag.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt.score(X_test, y_test))\n",
        "print(\"Bagging Accuracy:\", bag.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSX5bQmqIk5s",
        "outputId": "76a971ce-c767-4907-bce2-10aa3ed69a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9181286549707602\n",
            "Bagging Accuracy: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "wZtA_L8RIsZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "pred = rf.predict(X_test)\n",
        "disp = ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "WMJcQpUTI0U4",
        "outputId": "facf71e8-8c02-4f84-ef74-34267d473a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAGwCAYAAACuFMx9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALoNJREFUeJzt3Xt0FPX5x/HPJCEXIAkEJCEQMIhcRVCwNF6hpgZsEX7QWiy2ERFbuUMRoRUUEGKxKkYRrBcuPeC9pEIt/vhFBSmBFhCrFSKXKBFI0CKEBHNhd35/IGtXQHczk2x25v06Z85xv3PZJ54cnjzP9zszhmmapgAAgGNFhDoAAABQt0j2AAA4HMkeAACHI9kDAOBwJHsAAByOZA8AgMOR7AEAcLioUAdghdfr1aFDhxQfHy/DMEIdDgAgSKZp6sSJE0pNTVVERN3Vn5WVlaqurrZ8nejoaMXGxtoQUf0K62R/6NAhpaWlhToMAIBFxcXFatu2bZ1cu7KyUuntm6rkiMfytVJSUlRUVBR2CT+sk318fLwkKfX3MxQRF17/44FAdZq+K9QhAHXmlFmjjV++6vv3vC5UV1er5IhHn2y/UAnxte8elJ3wqn3vj1VdXU2yr09nWvcRcbEkezhWlBEd6hCAOlcfU7FN4w01ja/993gVvtPFYZ3sAQAIlMf0ymPhbTAe02tfMPWMZA8AcAWvTHlV+2xv5dxQ49Y7AAAcjsoeAOAKXnllpRFv7ezQItkDAFzBY5rymLVvxVs5N9Ro4wMA4HBU9gAAV3DzAj2SPQDAFbwy5XFpsqeNDwCAw1HZAwBcgTY+AAAOx2p8AADgWFT2AABX8H61WTk/XJHsAQCu4LG4Gt/KuaFGsgcAuILHlMW33tkXS31jzh4AAIejsgcAuAJz9gAAOJxXhjwyLJ0frmjjAwBQBzZu3KhBgwYpNTVVhmEoLy/Pb79pmpo1a5Zat26tuLg4ZWZmas+ePX7HHD16VCNGjFBCQoKaNWumUaNGqby8POhYSPYAAFfwmta3YFRUVKhnz55atGjROfcvWLBAubm5WrJkibZu3aomTZooKytLlZWVvmNGjBihf//731q/fr3Wrl2rjRs36s477wz6Z6eNDwBwBY/FNn6w5w4cOFADBw485z7TNLVw4ULde++9Gjx4sCRpxYoVSk5OVl5enoYPH65du3Zp3bp1+uc//6k+ffpIkh5//HHdeOON+sMf/qDU1NSAY6GyBwAgCGVlZX5bVVVV0NcoKipSSUmJMjMzfWOJiYnq27evCgoKJEkFBQVq1qyZL9FLUmZmpiIiIrR169agvo9kDwBwhTOVvZVNktLS0pSYmOjbcnJygo6lpKREkpScnOw3npyc7NtXUlKiVq1a+e2PiopSUlKS75hA0cYHALiC1zTkNS2sxv/q3OLiYiUkJPjGY2JiLMdW16jsAQAIQkJCgt9Wm2SfkpIiSSotLfUbLy0t9e1LSUnRkSNH/PafOnVKR48e9R0TKJI9AMAV7Grj2yE9PV0pKSnKz8/3jZWVlWnr1q3KyMiQJGVkZOjYsWPavn2775g333xTXq9Xffv2Der7aOMDAFzBowh5LNS4niCPLy8v1969e32fi4qKtHPnTiUlJaldu3aaNGmSHnjgAV188cVKT0/XzJkzlZqaqiFDhkiSunbtqgEDBmj06NFasmSJampqNG7cOA0fPjyolfgSyR4A4BKmxTl7M8hzt23bpv79+/s+T5kyRZKUnZ2tZcuWadq0aaqoqNCdd96pY8eO6eqrr9a6desUGxvrO2flypUaN26crr/+ekVERGjYsGHKzc0NOnaSPQAAdaBfv34yzfM/iccwDM2ZM0dz5sw57zFJSUlatWqV5VhI9gAAV6jvh+o0JCR7AIAreMwIeUwLc/a8zx4AADRUVPYAAFfwypDXQo3rVfiW9iR7AIAruHnOnjY+AAAOR2UPAHAF6wv0aOMDANCgnZ6zt/AiHNr4AACgoaKyBwC4gtfis/FZjQ8AQAPHnD0AAA7nVYRr77Nnzh4AAIejsgcAuILHNOSx8IpbK+eGGskeAOAKHosL9Dy08QEAQENFZQ8AcAWvGSGvhdX4XlbjAwDQsNHGBwAAjkVlDwBwBa+sraj32hdKvSPZAwBcwfpDdcK3GR6+kQMAgIBQ2QMAXMH6s/HDtz4m2QMAXMHN77Mn2QMAXMHNlX34Rg4AAAJCZQ8AcAXrD9UJ3/qYZA8AcAWvachr5T77MH7rXfj+mQIAAAJCZQ8AcAWvxTZ+OD9Uh2QPAHAF62+9C99kH76RAwCAgFDZAwBcwSNDHgsPxrFybqiR7AEArkAbHwAAOBaVPQDAFTyy1or32BdKvSPZAwBcwc1tfJI9AMAVeBEOAABwLCp7AIArmBbfZ29y6x0AAA0bbXwAAOBYVPYAAFdw8ytuSfYAAFfwWHzrnZVzQy18IwcAAAGhsgcAuAJtfAAAHM6rCHktNLStnBtq4Rs5AAAICJU9AMAVPKYhj4VWvJVzQ41kDwBwBebsAQBwONPiW+9MnqAHAAAaKip7AIAreGTIY+FlNlbODTWSPQDAFbymtXl3r2ljMPWMNj4AAA5HZY+ztHjtoFqsPeQ3Vp0cq4/n9pAkNTpSqQteKVbs3nIZp7w62T1RR25pL09Co1CEC9jup786qNvvPqC8pSl6al56qMOBTbwWF+hZOTfUSPY4p6rUOH06ubPv85nfcaPKozYLP1JVWpw+nXJ6f8u/HFSbJ/bowPSuUkT4zmkBktSpR7luHF6q/bsahzoU2MwrQ14L8+5Wzg21BvFnyqJFi3ThhRcqNjZWffv21T/+8Y9Qh+R6ZoTkSWzk27zxp6v2uL3lavSfKpXe1kHVbRurum1jlYxMV8wnFWq8uyzEUQPWxDb26O5H9uix33VQeRm1EJwj5Mn+xRdf1JQpU3Tfffdpx44d6tmzp7KysnTkyJFQh+Zq0Ueq1OHunbrwt/9SyjP7FPWfKkmSccqUDMmM+vovXLNRhGSc/kMACGdj7y/SP99urp2bm4U6FNSBM0/Qs7KFq5An+0ceeUSjR4/WyJEj1a1bNy1ZskSNGzfWc889F+rQXOvL9CYquS1dn07spCMj2qvR51VKe2i3jEqPKjs0kTc6Ui3//KmMKo+MKo9avlIswytFHq8JdehArV33o891UfdyLX2oXahDQR05M2dvZQuGx+PRzJkzlZ6erri4OF100UWaO3euTPPrZf2maWrWrFlq3bq14uLilJmZqT179tj9o4d2zr66ulrbt2/XjBkzfGMRERHKzMxUQUHBWcdXVVWpqqrK97msjLZxXTjZo5nvv6vbSpXpTZQ+/V+K33ZUZVdfoMO/ukitVn6iZm+WSoZ04ooWqmzXuAH86QjUTsvWVfrVzI/12+yuqqnmFxn2+P3vf6/Fixdr+fLl6t69u7Zt26aRI0cqMTFREyZMkCQtWLBAubm5Wr58udLT0zVz5kxlZWXpww8/VGxsrG2xhDTZf/755/J4PEpOTvYbT05O1u7du886PicnR7Nnz66v8PAVb+Mo1STHKPpIpSTpZPdEfTz/UkWcqJEiDXkbR6nD1HdV0zIpxJECtXNx9wo1b1mjJ/7yL99YZJR0yRVlGvSLEt3U7fvyesO3hYvTvLL4bPyvFuh9s9CMiYlRTEzMWcdv3rxZgwcP1o9+9CNJ0oUXXqjnn3/ety7NNE0tXLhQ9957rwYPHixJWrFihZKTk5WXl6fhw4fXOtZvCqs/YWfMmKHjx4/7tuLi4lCH5ApGpUeNPqvSqcRov3FvfCN5G0cpbneZIk+cUnnPZqEJELBoZ0Gifj2wp8YO+nr76F9N9NZrLTV2UE8SvUOYX63Gr+1mfpXs09LSlJiY6NtycnLO+X1XXnml8vPz9dFHH0mS3nvvPW3atEkDBw6UJBUVFamkpESZmZm+cxITE9W3b99zdretCGll37JlS0VGRqq0tNRvvLS0VCkpKWcdf76/nmCvli8fUMWlzVTTIkZRx6vV4rVDMiMMnfje6co94e+fqbp1nDxNoxS7v1ytXjygLzKTVZMSF+LIgdr5siJSn+zxv9Wu8stInfgi6qxxhC+73npXXFyshIQE3/j58tL06dNVVlamLl26KDIyUh6PR/PmzdOIESMkSSUlJZJ0zu72mX12CWmyj46OVu/evZWfn68hQ4ZIkrxer/Lz8zVu3LhQhuZqUV/UqPUz+xVRcUqeplH6smO8iqd3leer2++iSyvVcvWniqzwqKZFtP5zY6qOZSZ/x1UBwBkSEhL8kv35vPTSS1q5cqVWrVql7t27a+fOnZo0aZJSU1OVnZ1dD5F+LeQ3kk6ZMkXZ2dnq06ePvve972nhwoWqqKjQyJEjQx2aa5XcedG37v98aJo+H5pWT9EAoXHPiO6hDgE2q+8n6N19992aPn26b+69R48e+uSTT5STk6Ps7GxfB7u0tFStW7f2nVdaWqpevXrVOs5zCXmy/9nPfqbPPvtMs2bNUklJiXr16qV169ad1dYAAMAKu9r4gTp58qQiIvz/QIiMjJTX65UkpaenKyUlRfn5+b7kXlZWpq1bt+quu+6qdZznEvJkL0njxo2jbQ8AcJRBgwZp3rx5ateunbp37653331XjzzyiG6//XZJkmEYmjRpkh544AFdfPHFvlvvUlNTfVPbdmkQyR4AgLpW38/Gf/zxxzVz5kyNGTNGR44cUWpqqn71q19p1qxZvmOmTZumiooK3XnnnTp27JiuvvpqrVu3ztZ77CWSPQDAJeq7jR8fH6+FCxdq4cKF5z3GMAzNmTNHc+bMqXVcgQir++wBAEDwqOwBAK5Q35V9Q0KyBwC4gpuTPW18AAAcjsoeAOAKbq7sSfYAAFcwFfztc988P1yR7AEAruDmyp45ewAAHI7KHgDgCm6u7En2AABXcHOyp40PAIDDUdkDAFzBzZU9yR4A4Aqmaci0kLCtnBtqtPEBAHA4KnsAgCvU9/vsGxKSPQDAFdw8Z08bHwAAh6OyBwC4gpsX6JHsAQCu4OY2PskeAOAKbq7smbMHAMDhqOwBAK5gWmzjh3NlT7IHALiCKck0rZ0frmjjAwDgcFT2AABX8MqQwRP0AABwLlbjAwAAx6KyBwC4gtc0ZPBQHQAAnMs0La7GD+Pl+LTxAQBwOCp7AIAruHmBHskeAOAKJHsAABzOzQv0mLMHAMDhqOwBAK7g5tX4JHsAgCucTvZW5uxtDKae0cYHAMDhqOwBAK7AanwAABzOlLV30odxF582PgAATkdlDwBwBdr4AAA4nYv7+CR7AIA7WKzsFcaVPXP2AAA4HJU9AMAVeIIeAAAO5+YFerTxAQBwOCp7AIA7mIa1RXZhXNmT7AEAruDmOXva+AAAOByVPQDAHXioDgAAzubm1fgBJfvXXnst4AvedNNNtQ4GAADYL6BkP2TIkIAuZhiGPB6PlXgAAKg7YdyKtyKgZO/1eus6DgAA6pSb2/iWVuNXVlbaFQcAAHXLtGELU0Ene4/Ho7lz56pNmzZq2rSp9u/fL0maOXOmnn32WdsDBAAA1gSd7OfNm6dly5ZpwYIFio6O9o1fcskleuaZZ2wNDgAA+xg2bOEp6GS/YsUK/fGPf9SIESMUGRnpG+/Zs6d2795ta3AAANgmBG38gwcP6tZbb1WLFi0UFxenHj16aNu2bV+HZJqaNWuWWrdurbi4OGVmZmrPnj0WfshzCzrZHzx4UB07djxr3Ov1qqamxpagAAAId1988YWuuuoqNWrUSH/729/04Ycf6uGHH1bz5s19xyxYsEC5ublasmSJtm7dqiZNmigrK8v2NXFBP1SnW7dueuedd9S+fXu/8VdeeUWXXXaZbYEBAGCren6C3u9//3ulpaVp6dKlvrH09PSvL2eaWrhwoe69914NHjxY0unueXJysvLy8jR8+HALwfoLOtnPmjVL2dnZOnjwoLxer/785z+rsLBQK1as0Nq1a20LDAAAW9n01ruysjK/4ZiYGMXExJx1+GuvvaasrCz99Kc/1YYNG9SmTRuNGTNGo0ePliQVFRWppKREmZmZvnMSExPVt29fFRQU2Jrsg27jDx48WGvWrNH//d//qUmTJpo1a5Z27dqlNWvW6Ic//KFtgQEA0BClpaUpMTHRt+Xk5JzzuP3792vx4sW6+OKL9cYbb+iuu+7ShAkTtHz5cklSSUmJJCk5OdnvvOTkZN8+u9Tq2fjXXHON1q9fb2sgAADUJbtecVtcXKyEhATf+Lmqeun0WrY+ffpo/vz5kqTLLrtMH3zwgZYsWaLs7OzaB1ILtX4RzrZt27Rr1y5Jp+fxe/fubVtQAADYzqY5+4SEBL9kfz6tW7dWt27d/Ma6du2qV199VZKUkpIiSSotLVXr1q19x5SWlqpXr14WAj1b0Mn+008/1S233KK///3vatasmSTp2LFjuvLKK/XCCy+obdu2tgYIAEA4uuqqq1RYWOg39tFHH/kWuKenpyslJUX5+fm+5F5WVqatW7fqrrvusjWWoOfs77jjDtXU1GjXrl06evSojh49ql27dsnr9eqOO+6wNTgAAGxzZoGelS0IkydP1pYtWzR//nzt3btXq1at0h//+EeNHTtW0umXx02aNEkPPPCAXnvtNb3//vv65S9/qdTU1IBfQBeooCv7DRs2aPPmzercubNvrHPnznr88cd1zTXX2BocAAB2MczTm5Xzg3HFFVdo9erVmjFjhubMmaP09HQtXLhQI0aM8B0zbdo0VVRU6M4779SxY8d09dVXa926dYqNja19oOcQdLJPS0s758NzPB6PUlNTbQkKAADb1fN99pL04x//WD/+8Y/Pu98wDM2ZM0dz5syxENh3C7qN/9BDD2n8+PF+j/vbtm2bJk6cqD/84Q+2BgcAAKwLqLJv3ry5DOPruYqKigr17dtXUVGnTz916pSioqJ0++232z7PAACALWx6qE44CijZL1y4sI7DAACgjoWgjd9QBJTs6/vmfwAAYJ9aP1RHkiorK1VdXe03FsiDBgAAqHcuruyDXqBXUVGhcePGqVWrVmrSpImaN2/utwEA0CCF4H32DUXQyX7atGl68803tXjxYsXExOiZZ57R7NmzlZqaqhUrVtRFjAAAwIKg2/hr1qzRihUr1K9fP40cOVLXXHONOnbsqPbt22vlypV+DwsAAKDBcPFq/KAr+6NHj6pDhw6STs/PHz16VJJ09dVXa+PGjfZGBwCATc48Qc/KFq6CTvYdOnRQUVGRJKlLly566aWXJJ2u+M+8GAcAADQcQSf7kSNH6r333pMkTZ8+XYsWLVJsbKwmT56su+++2/YAAQCwhYsX6AU9Zz958mTff2dmZmr37t3avn27OnbsqEsvvdTW4AAAgHWW7rOXpPbt2/vezQsAQENlyOJb72yLpP4FlOxzc3MDvuCECRNqHQwAALBfQMn+0UcfDehihmGEJNl3nLBDUUajev9eoD787dDOUIcA1JmyE14171RPX+biW+8CSvZnVt8DABC2eFwuAABwKssL9AAACAsuruxJ9gAAV7D6FDxXPUEPAACEFyp7AIA7uLiNX6vK/p133tGtt96qjIwMHTx4UJL0pz/9SZs2bbI1OAAAbOPix+UGnexfffVVZWVlKS4uTu+++66qqqokScePH9f8+fNtDxAAAFgTdLJ/4IEHtGTJEj399NNq1OjrB9lcddVV2rFjh63BAQBgFze/4jboOfvCwkJde+21Z40nJibq2LFjdsQEAID9XPwEvaAr+5SUFO3du/es8U2bNqlDhw62BAUAgO2Ysw/c6NGjNXHiRG3dulWGYejQoUNauXKlpk6dqrvuuqsuYgQAABYE3cafPn26vF6vrr/+ep08eVLXXnutYmJiNHXqVI0fP74uYgQAwDI3P1Qn6GRvGIZ+97vf6e6779bevXtVXl6ubt26qWnTpnURHwAA9nDxffa1fqhOdHS0unXrZmcsAACgDgSd7Pv37y/DOP+KxDfffNNSQAAA1Amrt8+5qbLv1auX3+eamhrt3LlTH3zwgbKzs+2KCwAAe9HGD9yjjz56zvH7779f5eXllgMCAAD2su2td7feequee+45uy4HAIC9XHyfvW1vvSsoKFBsbKxdlwMAwFbceheEoUOH+n02TVOHDx/Wtm3bNHPmTNsCAwAA9gg62ScmJvp9joiIUOfOnTVnzhzdcMMNtgUGAADsEVSy93g8GjlypHr06KHmzZvXVUwAANjPxavxg1qgFxkZqRtuuIG32wEAwo6bX3Eb9Gr8Sy65RPv376+LWAAAQB0IOtk/8MADmjp1qtauXavDhw+rrKzMbwMAoMFy4W13UhBz9nPmzNFvfvMb3XjjjZKkm266ye+xuaZpyjAMeTwe+6MEAMAqF8/ZB5zsZ8+erV//+td666236jIeAABgs4CTvWme/pPmuuuuq7NgAACoKzxUJ0Df9rY7AAAaNNr4genUqdN3JvyjR49aCggAANgrqGQ/e/bss56gBwBAOKCNH6Dhw4erVatWdRULAAB1x8Vt/IDvs2e+HgCA8BT0anwAAMKSiyv7gJO91+utyzgAAKhTzNkDAOB0Lq7sg342PgAACC9U9gAAd3BxZU+yBwC4gpvn7GnjAwDgcFT2AAB3oI0PAICz0cYHAACORbIHALiDacNWSw8++KAMw9CkSZN8Y5WVlRo7dqxatGihpk2batiwYSotLa39l3wLkj0AwB1ClOz/+c9/6qmnntKll17qNz558mStWbNGL7/8sjZs2KBDhw5p6NChtfuS70CyBwCgjpSXl2vEiBF6+umn1bx5c9/48ePH9eyzz+qRRx7RD37wA/Xu3VtLly7V5s2btWXLFtvjINkDAFzBsGGTpLKyMr+tqqrqvN85duxY/ehHP1JmZqbf+Pbt21VTU+M33qVLF7Vr104FBQV2/Lh+SPYAAHewqY2flpamxMRE35aTk3POr3vhhRe0Y8eOc+4vKSlRdHS0mjVr5jeenJyskpISqz/pWbj1DgDgCnbdeldcXKyEhATfeExMzFnHFhcXa+LEiVq/fr1iY2Nr/6U2obIHACAICQkJftu5kv327dt15MgRXX755YqKilJUVJQ2bNig3NxcRUVFKTk5WdXV1Tp27JjfeaWlpUpJSbE9Zip7AIA71OMT9K6//nq9//77fmMjR45Uly5ddM899ygtLU2NGjVSfn6+hg0bJkkqLCzUgQMHlJGRYSHIcyPZAwDco56eghcfH69LLrnEb6xJkyZq0aKFb3zUqFGaMmWKkpKSlJCQoPHjxysjI0Pf//73bY+HZA8AQAg8+uijioiI0LBhw1RVVaWsrCw9+eSTdfJdJHsAgCuE+tn4b7/9tt/n2NhYLVq0SIsWLbJ24QCQ7AEA7uDit96xGh8AAIejsgcAuEKo2/ihRLIHALgDbXwAAOBUVPYAAFegjQ8AgNO5uI1PsgcAuIOLkz1z9gAAOByVPQDAFZizBwDA6WjjAwAAp6KyBwC4gmGaMszal+dWzg01kj0AwB1o4wMAAKeisgcAuAKr8QEAcDra+AAAwKmo7AEArkAbHwAAp3NxG59kDwBwBTdX9szZAwDgcFT2AAB3oI0PAIDzhXMr3gra+AAAOByVPQDAHUzz9Gbl/DBFsgcAuAKr8QEAgGNR2QMA3IHV+AAAOJvhPb1ZOT9c0cYHAMDhqOwRkEv6luunYz7TxT1OqkXKKd1/+4UqWJcY6rCAgLy/pYlefrKV9rzfWEdLG+m+Z4t05cDjvv2mKa14KEXrVrVQeVmkuvWp0IQHi9WmQ7Uk6b3NTTXtJx3Pee3c1wvVudeX9fJzwCIXt/Gp7BGQ2MZe7f93rJ74bdtQhwIErfJkhDp0/1Lj5n96zv0vLWqlvzx3gcY/WKzH1n6k2MZe/fbnF6m60pAkdetToed3fuC3Dfj5f5TSrkqdepLow8WZ1fhWtnAV0mS/ceNGDRo0SKmpqTIMQ3l5eaEMB99i21sJWr6gtTZTzSMMXfGDE7rtnhJd9V/V/BmmKeU9c4FumViiKweUqUO3Sk3L/UT/KW3k+31vFG0qqdUp35bQ/JQK3kjQDT87KsOo758GtXbmPnsrW5gKabKvqKhQz549tWjRolCGAcDFSg5E6+iRRrr8mnLfWJMEr7pcdlK7tjc55zkF/5uoE19E6YafHa2vMAFLQjpnP3DgQA0cODDg46uqqlRVVeX7XFZWVhdhAXCRo0dO/zPY7IIav/FmF9T49n3TG8+3UO9+J3RBas0596Nh4qE6YSInJ0eJiYm+LS0tLdQhAXCZzw410va345V1y39CHQqCZdqwhamwSvYzZszQ8ePHfVtxcXGoQwIQ5pJanZIkHfuskd/4sc8a+fb9t/99MUnxzU8p44az5/+Bhiqskn1MTIwSEhL8NgCwIqVdtZJa1ejdTU19YxUnIrT73cbq2rvC71jTPJ3sM3/yhaIaffNKaOjcvBqf++wRkNjGHqWmV/s+p6RVq0P3L3XiWKQ+OxgdwsiA7/ZlRYQOFcX4PpcUR2vfB3GKb3ZKrdrWaMgdn+n5x5LVJr1KKe2qtXxBa7VIrtGVA/yr952bmqrkQIwG/JwWfljirXfAt+vU80s99Oo+3+dfzz4kSfrfF5vr4cntQhUWEJCP3mvs91Ccp+5vI0n64c1HNXXhAd089ogqT0bosWlpKi+LVPcrKjRv5X5Fx/r/477u+Rbq1qdc7S6uEhBOQprsy8vLtXfvXt/noqIi7dy5U0lJSWrXjgTSkPyroKmyUnuGOgygVnpeWa43Du08737DkLKnlSh7Wsm3XmfGk5/YHBnqk5tX44c02W/btk39+/f3fZ4yZYokKTs7W8uWLQtRVAAAR3Lx43JDmuz79esnM4znQAAACAfM2QMAXIE2PgAATuc1T29Wzg9TJHsAgDu4eM4+rB6qAwAAgkdlDwBwBUMW5+xti6T+kewBAO7g4ifo0cYHAMDhqOwBAK7ArXcAADgdq/EBAIBTUdkDAFzBME0ZFhbZWTk31Ej2AAB38H61WTk/TNHGBwDA4ajsAQCuQBsfAACnYzU+AAAOd+YJela2IOTk5OiKK65QfHy8WrVqpSFDhqiwsNDvmMrKSo0dO1YtWrRQ06ZNNWzYMJWWltr5U0si2QMAUCc2bNigsWPHasuWLVq/fr1qamp0ww03qKKiwnfM5MmTtWbNGr388svasGGDDh06pKFDh9oeC218AIAr2PUEvbKyMr/xmJgYxcTEnHX8unXr/D4vW7ZMrVq10vbt23Xttdfq+PHjevbZZ7Vq1Sr94Ac/kCQtXbpUXbt21ZYtW/T973+/9sF+A5U9AMAdbGrjp6WlKTEx0bfl5OQE9PXHjx+XJCUlJUmStm/frpqaGmVmZvqO6dKli9q1a6eCggJbf3QqewAAglBcXKyEhATf53NV9d/k9Xo1adIkXXXVVbrkkkskSSUlJYqOjlazZs38jk1OTlZJSYmtMZPsAQCuYHhPb1bOl6SEhAS/ZB+IsWPH6oMPPtCmTZtqH4AFtPEBAO5Qz6vxzxg3bpzWrl2rt956S23btvWNp6SkqLq6WseOHfM7vrS0VCkpKVZ+0rOQ7AEAqAOmaWrcuHFavXq13nzzTaWnp/vt7927txo1aqT8/HzfWGFhoQ4cOKCMjAxbY6GNDwBwh3p+qM7YsWO1atUq/eUvf1F8fLxvHj4xMVFxcXFKTEzUqFGjNGXKFCUlJSkhIUHjx49XRkaGrSvxJZI9AMAl6vtxuYsXL5Yk9evXz2986dKluu222yRJjz76qCIiIjRs2DBVVVUpKytLTz75ZK1jPB+SPQAAdcAM4I+D2NhYLVq0SIsWLarTWEj2AAB3sLDIznd+mCLZAwDcwZS1d9KHb64n2QMA3MHNr7jl1jsAAByOyh4A4A6mLM7Z2xZJvSPZAwDcwcUL9GjjAwDgcFT2AAB38EoyLJ4fpkj2AABXYDU+AABwLCp7AIA7uHiBHskeAOAOLk72tPEBAHA4KnsAgDu4uLIn2QMA3IFb7wAAcDZuvQMAAI5FZQ8AcAfm7AEAcDivKRkWErY3fJM9bXwAAByOyh4A4A608QEAcDqLyV7hm+xp4wMA4HBU9gAAd6CNDwCAw3lNWWrFsxofAAA0VFT2AAB3ML2nNyvnhymSPQDAHZizBwDA4ZizBwAATkVlDwBwB9r4AAA4nCmLyd62SOodbXwAAByOyh4A4A608QEAcDivV5KFe+W94XufPW18AAAcjsoeAOAOtPEBAHA4Fyd72vgAADgclT0AwB1c/Lhckj0AwBVM0yvTwpvrrJwbaiR7AIA7mKa16pw5ewAA0FBR2QMA3MG0OGcfxpU9yR4A4A5er2RYmHcP4zl72vgAADgclT0AwB1o4wMA4Gym1yvTQhs/nG+9o40PAIDDUdkDANyBNj4AAA7nNSXDncmeNj4AAA5HZQ8AcAfTlGTlPvvwrexJ9gAAVzC9pkwLbXyTZA8AQANnemWtsufWOwAA0EBR2QMAXIE2PgAATufiNn5YJ/szf2WdUo2l5yQADVnZifD9Bwb4LmXlp3+/66NqtporTqnGvmDqWVgn+xMnTkiSNun1EEcC1J3mnUIdAVD3Tpw4ocTExDq5dnR0tFJSUrSpxHquSElJUXR0tA1R1S/DDONJCK/Xq0OHDik+Pl6GYYQ6HFcoKytTWlqaiouLlZCQEOpwAFvx+13/TNPUiRMnlJqaqoiIulszXllZqerqasvXiY6OVmxsrA0R1a+wruwjIiLUtm3bUIfhSgkJCfxjCMfi97t+1VVF/99iY2PDMknbhVvvAABwOJI9AAAOR7JHUGJiYnTfffcpJiYm1KEAtuP3G04V1gv0AADAd6OyBwDA4Uj2AAA4HMkeAACHI9kDAOBwJHsEbNGiRbrwwgsVGxurvn376h//+EeoQwJssXHjRg0aNEipqakyDEN5eXmhDgmwFckeAXnxxRc1ZcoU3XfffdqxY4d69uyprKwsHTlyJNShAZZVVFSoZ8+eWrRoUahDAeoEt94hIH379tUVV1yhJ554QtLp9xKkpaVp/Pjxmj59eoijA+xjGIZWr16tIUOGhDoUwDZU9vhO1dXV2r59uzIzM31jERERyszMVEFBQQgjAwAEgmSP7/T555/L4/EoOTnZbzw5OVklJSUhigoAECiSPQAADkeyx3dq2bKlIiMjVVpa6jdeWlqqlJSUEEUFAAgUyR7fKTo6Wr1791Z+fr5vzOv1Kj8/XxkZGSGMDAAQiKhQB4DwMGXKFGVnZ6tPnz763ve+p4ULF6qiokIjR44MdWiAZeXl5dq7d6/vc1FRkXbu3KmkpCS1a9cuhJEB9uDWOwTsiSee0EMPPaSSkhL16tVLubm56tu3b6jDAix7++231b9//7PGs7OztWzZsvoPCLAZyR4AAIdjzh4AAIcj2QMA4HAkewAAHI5kDwCAw5HsAQBwOJI9AAAOR7IHAMDhSPYAADgcyR6w6LbbbtOQIUN8n/v166dJkybVexxvv/22DMPQsWPHznuMYRjKy8sL+Jr333+/evXqZSmujz/+WIZhaOfOnZauA6D2SPZwpNtuu02GYcgwDEVHR6tjx46aM2eOTp06Veff/ec//1lz584N6NhAEjQAWMWLcOBYAwYM0NKlS1VVVaXXX39dY8eOVaNGjTRjxoyzjq2urlZ0dLQt35uUlGTLdQDALlT2cKyYmBilpKSoffv2uuuuu5SZmanXXntN0tet93nz5ik1NVWdO3eWJBUXF+vmm29Ws2bNlJSUpMGDB+vjjz/2XdPj8WjKlClq1qyZWrRooWnTpumbr5f4Zhu/qqpK99xzj9LS0hQTE6OOHTvq2Wef1ccff+x7+Urz5s1lGIZuu+02SadfIZyTk6P09HTFxcWpZ8+eeuWVV/y+5/XXX1enTp0UFxen/v37+8UZqHvuuUedOnVS48aN1aFDB82cOVM1NTVnHffUU08pLS1NjRs31s0336zjx4/77X/mmWfUtWtXxcbGqkuXLnryySeDjgVA3SHZwzXi4uJUXV3t+5yfn6/CwkKtX79ea9euVU1NjbKyshQfH6933nlHf//739W0aVMNGDDAd97DDz+sZcuW6bnnntOmTZt09OhRrV69+lu/95e//KWef/555ebmateuXXrqqafUtGlTpaWl6dVXX5UkFRYW6vDhw3rsscckSTk5OVqxYoWWLFmif//735o8ebJuvfVWbdiwQdLpP0qGDh2qQYMGaefOnbrjjjs0ffr0oP+fxMfHa9myZfrwww/12GOP6emnn9ajjz7qd8zevXv10ksvac2aNVq3bp3effddjRkzxrd/5cqVmjVrlubNm6ddu3Zp/vz5mjlzppYvXx50PADqiAk4UHZ2tjl48GDTNE3T6/Wa69evN2NiYsypU6f69icnJ5tVVVW+c/70pz+ZnTt3Nr1er2+sqqrKjIuLM9944w3TNE2zdevW5oIFC3z7a2pqzLZt2/q+yzRN87rrrjMnTpxomqZpFhYWmpLM9evXnzPOt956y5RkfvHFF76xyspKs3HjxubmzZv9jh01apR5yy23mKZpmjNmzDC7devmt/+ee+4561rfJMlcvXr1efc/9NBDZu/evX2f77vvPjMyMtL89NNPfWN/+9vfzIiICPPw4cOmaZrmRRddZK5atcrvOnPnzjUzMjJM0zTNoqIiU5L57rvvnvd7AdQt5uzhWGvXrlXTpk1VU1Mjr9ern//857r//vt9+3v06OE3T//ee+9p7969io+P97tOZWWl9u3bp+PHj+vw4cPq27evb19UVJT69OlzViv/jJ07dyoyMlLXXXddwHHv3btXJ0+e1A9/+EO/8erqal122WWSpF27dvnFIUkZGRkBf8cZL774onJzc7Vv3z6Vl5fr1KlTSkhI8DumXbt2atOmjd/3eL1eFRYWKj4+Xvv27dOoUaM0evRo3zGnTp1SYmJi0PEAqBskezhW//79tXjxYkVHRys1NVVRUf6/7k2aNPH7XF5ert69e2vlypVnXeuCCy6oVQxxcXFBn1NeXi5J+utf/+qXZKXT6xDsUlBQoBEjRmj27NnKyspSYmKiXnjhBT388MNBx/r000+f9cdHZGSkbbECsIZkD8dq0qSJOnbsGPDxl19+uV588UW1atXqrOr2jNatW2vr1q269tprJZ2uYLdv367LL7/8nMf36NFDXq9XGzZsUGZm5ln7z3QWPB6Pb6xbt26KiYnRgQMHztsR6Nq1q2+x4Rlbtmz57h/yv2zevFnt27fX7373O9/YJ598ctZxBw4c0KFDh5Samur7noiICHXu3FnJyclKTU3V/v37NWLEiKC+H0D9YYEe8JURI0aoZcuWGjx4sN555x0VFRXp7bff1oQJE/Tpp59KkiZOnKgHH3xQeXl52r17t8aMGfOt98hfeOGFys7O1u233668vDzfNV966SVJUvv27WUYhtauXavPPvtM5eXlio+P19SpUzV58mQtX75c+/bt044dO/T444/7Fr39+te/1p49e3T33XersLBQq1at0rJly4L6eS+++GIdOHBAL7zwgvbt26fc3NxzLjaMjY1Vdna23nvvPb3zzjuaMGGCbr75ZqWkpEiSZs+erZycHOXm5uqjjz7S+++/r6VLl+qRRx4JKh4AdYdkD3ylcePG2rhxo9q1a6ehQ4eqa9euGjVqlCorK32V/m9+8xv94he/UHZ2tjIyMhQfH6//+Z//+dbrLl68WD/5yU80ZswYdenSRaNHj1ZFRYUkqU2bNpo9e7amT5+u5ORkjRs3TpI0d+5czZw5Uzk5OeratasGDBigv/71r0pPT5d0eh791VdfVV5ennr27KklS5Zo/vz5Qf28N910kyZPnqxx48apV69e2rx5s2bOnHnWcR07dtTQoUN144036oYbbtCll17qd2vdHXfcoWeeeUZLly5Vjx49dN1112nZsmW+WAGEnmGeb2URAABwBCp7AAAcjmQPAIDDkewBAHA4kj0AAA5HsgcAwOFI9gAAOBzJHgAAhyPZAwDgcCR7AAAcjmQPAIDDkewBAHC4/wdfmpUHPfBFNgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36.Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "lPY-KjXpJwxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True)),\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STfcYFvsJ2n9",
        "outputId": "b1b359a2-ac48-42c0-e7cc-a522654222ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 0.9532163742690059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 37.Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "NBnM-lCGJ6iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "fi = rf.feature_importances_\n",
        "top5 = np.argsort(fi)[-5:]\n",
        "\n",
        "print(\"Top 5 Features:\")\n",
        "for idx in top5:\n",
        "    print(data.feature_names[idx], fi[idx])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udpMuaCfKBqt",
        "outputId": "1eda535e-7cdb-4681-fd26-5d97b76a49ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Features:\n",
            "worst radius 0.08416375548378313\n",
            "worst area 0.11014614686277549\n",
            "worst concave points 0.11552564712102359\n",
            "worst perimeter 0.14350975209554184\n",
            "mean concave points 0.16035614153616565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "OaM8AIk_KFss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "pred = bag.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx-5pxLrKKMT",
        "outputId": "8afe9095-5a26-4cdc-a22b-9597154b9f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        63\n",
            "           1       0.96      0.97      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "AiOY64NnKOhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for d in [None, 3, 5, 10]:\n",
        "    rf = RandomForestClassifier(max_depth=d, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(\"Max Depth:\", d, \"→ Accuracy:\", rf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FgBVQOoKUaS",
        "outputId": "bc0a0a10-6ace-44c5-9998-91b05f5ac8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: None → Accuracy: 0.9707602339181286\n",
            "Max Depth: 3 → Accuracy: 0.9707602339181286\n",
            "Max Depth: 5 → Accuracy: 0.9649122807017544\n",
            "Max Depth: 10 → Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance"
      ],
      "metadata": {
        "id": "HOnLEqNdKYtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the California Housing dataset for regression\n",
        "X_reg, y_reg = fetch_california_housing(return_X_y=True)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "models = {\n",
        "    \"DecisionTree\": DecisionTreeRegressor(),\n",
        "    \"KNN\": KNeighborsRegressor()\n",
        "}\n",
        "\n",
        "for name, est in models.items():\n",
        "    reg = BaggingRegressor(estimator=est, n_estimators=30)\n",
        "    reg.fit(X_train_reg, y_train_reg)\n",
        "    print(name, \"→ MSE:\", mean_squared_error(y_test_reg, reg.predict(X_test_reg)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZjZwRl_KUR7",
        "outputId": "a8124d34-3962-4451-ec8f-68df83f0b1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree → MSE: 0.2606356780065329\n",
            "KNN → MSE: 1.1018848441411782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score="
      ],
      "metadata": {
        "id": "gx6PLCpqLFwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "prob = rf.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok-ZdWp8LT35",
        "outputId": "5d8b208b-b8dc-4391-f59a-5d0ecd813b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.9966931216931216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validation."
      ],
      "metadata": {
        "id": "Ys3ivzIQLFnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(bag, X, y, cv=5)\n",
        "print(\"CV Scores:\", scores)\n",
        "print(\"Mean CV Score:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uOF1NsYLYuu",
        "outputId": "a027d501-9278-4aaf-ce64-45ad1658d909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CV Scores: [0.94736842 0.93859649 0.98245614 0.95614035 0.96460177]\n",
            "Mean CV Score: 0.9578326346840551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "43.Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "I1j4nUuxLmmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "probs = rf.predict_proba(X_test)[:,1]\n",
        "precision, recall, thresh = precision_recall_curve(y_test, probs)\n",
        "\n",
        "print(\"Precision values:\", precision)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DngPUwzwLYmc",
        "outputId": "0b2c614c-c3a9-41b7-daa9-0e5f7cbc1504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision values: [0.63157895 0.73469388 0.77697842 0.80597015 0.8372093  0.8503937\n",
            " 0.864      0.87804878 0.8852459  0.89256198 0.9        0.91525424\n",
            " 0.92307692 0.93103448 0.93913043 0.94736842 0.94690265 0.95535714\n",
            " 0.96396396 0.96363636 0.97247706 0.98148148 0.98130841 0.98113208\n",
            " 0.98095238 0.98076923 0.99029126 0.99019608 0.99009901 0.99\n",
            " 0.98989899 1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy="
      ],
      "metadata": {
        "id": "qSVrnVjZLzIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = [\n",
        "    ('rf', RandomForestClassifier()),\n",
        "]\n",
        "\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        ")\n",
        "\n",
        "stack.fit(X_train, y_train)\n",
        "print(\"Stacking Accuracy:\", stack.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkh_ORxSL3Da",
        "outputId": "9a138898-36a2-485c-bf95-72a00a312899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
      ],
      "metadata": {
        "id": "l_mbnOMbL8bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset for regression and define the _b variables\n",
        "X_b, y_b = fetch_california_housing(return_X_y=True)\n",
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.3, random_state=42)\n",
        "\n",
        "for sample in [0.3, 0.5, 0.7, 1.0]:\n",
        "    reg = BaggingRegressor(max_samples=sample, random_state=42)\n",
        "    reg.fit(X_train_b, y_train_b)\n",
        "    print(\"Bootstrap\", sample, \"→ MSE:\", mean_squared_error(y_test_b, reg.predict(X_test_b)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB2Xkd10MEV0",
        "outputId": "3e7f10a7-a223-49af-eebd-e1871743b519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap 0.3 → MSE: 0.31342293336380184\n",
            "Bootstrap 0.5 → MSE: 0.2968519387859436\n",
            "Bootstrap 0.7 → MSE: 0.2952169775521256\n",
            "Bootstrap 1.0 → MSE: 0.28623579601385674\n"
          ]
        }
      ]
    }
  ]
}